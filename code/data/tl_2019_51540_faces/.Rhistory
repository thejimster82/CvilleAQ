library(tidyverse)
df_train_tb = as.tibble(df_train)
decomposed_df = time_decompose(df_train_tb,target=PM, method = "stl", frequency = "auto", trend = "auto")
anomalized_df = anomalize(decomposed_df,target=remainder, method = "gesd", alpha = 0.05, max_anoms = 0.3)
plot_anomaly_decomposition(anomalized_df)
?anomalize
recomposed_df = time_recompose(anomalized_df)
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
recomposed_df
plot_anomalies(decomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
plot_anomalies(decomposed_df, ncol = 3, alpha_dots = 0.5)
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
?plot_anomalies
plot_anomalies(df_train_tb, ncol = 3, alpha_dots = 0.5)
df_train_tb
?time_recompose
recomposed_df = time_recompose(anomalized_df)
plot_anomalies(recomposed_df, ncol = 3, alpha_dots = 0.5)
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
After seeing our decomposed data and anomalies, we can use 'time_recompose' to reconstruct our data and generate confidence bands around our values using the trend and seasonal components. This allows us to see more specifically how the anomalies lie outside of all of the trends of our data.
```{r}
recomposed_df = time_recompose(anomalized_df)
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
```
After seeing our decomposed data and anomalies, we can use 'time_recompose' to reconstruct our data and generate confidence bands around our values using the trend and seasonal components. This allows us to see more specifically how the anomalies lie outside of all of the trends of our data.
```{r}
recomposed_df = time_recompose(anomalized_df)
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
```
knitr::opts_chunk$set(echo = TRUE)
library(rgl)
library(knitr)
rgl::setupKnitr()
knit_hooks$set(webgl = hook_webgl)
plot(density(df$Temp))
plot(density(df$Wind))
plot(density(df$Temp))
library(mclust)
help(mvn)
mvNorms = mvn(data=dplyr::select(df,PM,Temp,Wind))
mvNorms = mvn(modelName="Ellipsoidal",data=dplyr::select(df,PM,Temp,Wind))
mvNorms
library(mvtnorm)
help(dmvnorm)
library(mclust)
install.packages(c("openair", "useful"))
mvNorms$parameters$variance
mvNorms$parameters$mean
mvNormProbs = dmvnorm(dplyr::select(df,PM,Temp,Wind),mvNorms$parameters$mean,mvNorms$parameters$variance)
mvNormProbs = dmvnorm(dplyr::select(df,PM,Temp,Wind),mvNorms$parameters$mean,as.matrix(mvNorms$parameters$variance))
mvNormProbs = dmvnorm(dplyr::select(df,PM,Temp,Wind),as.matrix(mvNorms$parameters$mean),as.matrix(mvNorms$parameters$variance))
mvMeans = mvNorms$parameters$mean
mvVars = mvNorms$parameters$variance
mvNormProbs = dmvnorm(dplyr::select(df,PM,Temp,Wind),mvMeans,mvVars)
dplyr::select(df,PM,Temp,Wind)
as.matrix(dplyr::select(df,PM,Temp,Wind))
mvMeans = mvNorms$parameters$mean
mvVars = as.matrix(mvNorms$parameters$variance)
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
mvVars
mvNorms$parameters$variance
mvNorms$parameters$variance$Sigma
mvVars = as.matrix(mvNorms$parameters$variance$sigma)
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
as.matrix(mvNorms$parameters$variance$sigma)
mvVars = mvNorms$parameters$variance$sigma
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
help(as.matrix)
mvVars = as.matrix(mvNorms$parameters$variance$sigma,nrow=3,ncol=3)
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
mvVars
mvNorms$parameters$variance$sigma
as.matrix(mvNorms$parameters$variance$sigma,nrow=3,ncol=3)
mvNorms$parameters$variance$sigma
mvNorms$parameters$variance$Sigma
mvNorms$parameters$variance$cholSigma
str(mvNorms$parameters$variance$sigma)
mvNorms$parameters$variance$cholSigma
mvNorms$parameters$variance$sigma
as.data.frame(mvNorms$parameters$variance$sigma)
as.matrix(as.data.frame(mvNorms$parameters$variance$sigma))
as.matrix(as.data.frame(mvNorms$parameters$variance$sigma),nrow=3,ncol=3)
mvVars = as.matrix(as.data.frame(mvNorms$parameters$variance$sigma),nrow=3,ncol=3)
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
mvNormProbs
plot3d(dplyr::select(df,PM,Temp,Wind), col=ifelse(mvNormProbs<.025 | mvNormProbs>0.975,"red","green"))
plot(mvNormProbs)
quantile(mvNormProbs)
mvQuant = quantile(mvNormProbs,c(0.025,0.975))
mvQuant
mvQuant[1]
plot3d(dplyr::select(df,PM,Temp,Wind), col=ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,"red","green"))
plot3d(dplyr::select(df,PM,Temp,Wind), col=ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,"red","black"))
plot(density(isf$scores$anomaly_score),ylab = "Density",main="Anomaly Score")
plot.default(df$datetime, df$PM,col=ifelse(df$scores$anomaly_score>cutoff,"red","black"),ylab = "PM2.5 (ug/m^3)")
plot.default(df$datetime, df$PM,col=ifelse(df$scores$anomaly_score>cutoff,"red","black"),ylab = "PM2.5 (ug/m^3)")
plot(density(df$PM))
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
plot(density(df$PM))
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
plot(df$datetime, df$PM,col=ifelse(df$probs<.025 | df$probs>0.975,"red","black"))
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(rgl)
library(knitr)
rgl::setupKnitr()
knit_hooks$set(webgl = hook_webgl)
isfMV = isolationForest$new()
isfMV$fit(dplyr::select(df,PM,Wind,Temp))
density(isfMV$scores$anomaly_score)
isfMV$scores
isfMV$scores$anomaly_score
plot(density(isfMV$scores$anomaly_score),ylab = "Density",main="Anomaly Score")
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df$scores$anomaly_score>cutoff,"red","black"))
dplyr::select(df,PM,Wind,Temp)
isfMV = isolationForest$new()
isfMV$fit(dplyr::select(df,PM,Wind,Temp))
isfMV$fit(dplyr::select(df,PM,Wind,Temp))
plot(density(isfMV$scores$anomaly_score),ylab = "Density",main="Anomaly Score")
df$MVscores = isf$scores
#assume
cutoff = quantile(isf$scores$anomaly_score,c(0.975))
df$MVscores = isf$scores
df2
library(knitr)
rgl::setupKnitr()
knit_hooks$set(webgl = hook_webgl)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(rgl)
library(knitr)
rgl::setupKnitr()
knit_hooks$set(webgl = hook_webgl)
```{r, include=FALSE}
library(fitdistrplus)
library(fitdistrplus)
library(forecast)
library(mclust)
For this tutorial, we will be examining a dataset that is compiled from the Environmental Protection Agency's Outdoor Air Data Portal (https://www.epa.gov/outdoor-air-quality-data). Outdoor air quality data is collected all over the country from various state and local groups. In particular, we will be looking at particulate matter (of size 2.5 microns in diameter or less), wind speed, and temperature. We have combined different datasets with these features using an inner join, so each row has data for all of the columns.
knit_hooks$set(rgl = hook_rgl)
Naturally, with air quality data, it would be incredibly important to detect anomalies in the data, as there are strict regulations in place for keeping the quality of the air at healthy levels. Identifying measurements as anomalies can lead to further research as to whether the anomalies came from human/sensor error or whether there are actually harmful levels of pollutants in the air.
There are several methods of anomaly detection that we will walk though. Finally, we will look at anomaly detection through a time-series lens.
### Part 1: General Methods of Anomaly Detection
#### Section 1: K-means
The first anomaly detection technique we will try uses K-means. This is a popular clustering technique in data mining that involves creating k centroids to which observations are assigned based on which centroid they are nearest to.
How can we use this for anomaly detection?
Since typical observations will be close to their nearest centroids, points that are located furthest away from these clusters will be the most unique.
Let's go through it:
First, let's take a look at the data. Note that datetimes are stored as factors in CSV files, so we first need to convert the column back to a datetime format.
```{r}
df <- read.csv("epa.csv")
df$datetime <- as.POSIXct(df$datetime)
head(df)
```
As you can see, we have a variable called "datetime" which is of the POSIXct date-time class. It shows the date and the local time of the measurements, spanning from January 1, 2019 at 12:00am to May 5, 2019 at 11:00pm. We have our particulate matter (PM2.5) variable in units of micrograms per cubic meter. We have temperature in degrees Fahrenheit. Finally, we have wind speed in knots.
For this k-means algorithm, let's cluster based on PM, temperature, and wind. First let's decide how many clusters we want.
```{r}
n_cluster = 1:15   # set upper bound for number of clusters
bss <- 1:15   # between sum of squares for plotting
plot(1:15,bss)
From the plot, it appears that the between sum-of-squares error levels off after around 3 clusters. Let's proceed with that number and look at the resulting clusters.
```{r, webgl = TRUE}
km = kmeans(clust_df, 3)
plot3d(clust_df, col=km$cluster)
km = kmeans(clust_df, 3)
```{r, webgl = TRUE}
Now that we have the k-means centroids, we can calculate the Euclidean distance from each observation to its nearest centroid.
```{r}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(rgl)
library(knitr)
rgl::setupKnitr()
knit_hooks$set(webgl = hook_webgl)
library(stringr)
library(openair)
library(useful)
library(MASS)
library(e1071)
library(dplyr)
library(solitude)
library(anomalize)
library(tidyverse)
library(fitdistrplus)
library(forecast)
library(mclust)
knit_hooks$set(rgl = hook_rgl)
digits <- function(x, k=2) format(round(x, k), nsmall=k)
df <- read.csv("epa.csv")
df$datetime <- as.POSIXct(df$datetime)
head(df)
clust_df <- df[,2:4]
n_cluster = 1:15   # set upper bound for number of clusters
bss <- 1:15   # between sum of squares for plotting
# record between sum of squares for each number of clusters
for (i in n_cluster){
km = kmeans(clust_df, i)
bss[i] = km$betweenss
}
plot(1:15,bss)
km = kmeans(clust_df, 3)
km$centers
plot3d(clust_df, col=km$cluster)
eucDists <- sqrt(rowSums(clust_df - fitted(km)) ^ 2)
outlierFrac <- 0.02
numOutliers = as.integer(outlierFrac*length(eucDists))
threshold = min(tail(sort(as.matrix(eucDists)), numOutliers))
# All observations with distances greater than or equal to the threshold labeled as 1
df2 <- df # Creating a duplicate of the dataframe but adding classification columns
df2$km_anomaly = as.integer(eucDists >= threshold)
plot.default(df2$datetime, df2$PM, col=ifelse(df2$km_anomaly==1, "red", "black"), ylab = "PM2.5 (ug/m^3)")
plot.default(df2$datetime, df2$Temp, col=ifelse(df2$km_anomaly==1, "red", "black"), ylab = "Temperature (degrees Fahrenheit)")
plot.default(df2$datetime, df2$Wind, col=ifelse(df2$km_anomaly==1, "red", "black"), ylab = "Wind speed (knots)")
plot3d(clust_df, col=df2$km_anomaly+1)
df2
svm_df <- df[,2:4]
svmMod <- svm(svm_df,type='one-classification',kernel = "radial",gamma=0.1,nu=0.02)
pred_svm <- predict(svmMod,svm_df)
df2$svm_anomaly <- 1-as.integer(pred_svm)  # Converting to same labelling system as K-means (1's and 0's)
summary(pred_svm)  # "FALSE" represents the anomalies
plot.default(df2$datetime, df2$PM, col=ifelse(df2$svm_anomaly==1, "red", "black"), ylab = "PM2.5 (ug/m^3)")
plot.default(df2$datetime, df2$Temp, col=ifelse(df2$svm_anomaly==1, "red", "black"), ylab = "Temperature (degrees Fahrenheit)")
plot.default(df2$datetime, df2$Wind, col=ifelse(df2$svm_anomaly==1, "red", "black"), ylab = "Wind speed (knots)")
plot3d(svm_df, col=df2$svm_anomaly+1)
df2
# https://towardsdatascience.com/time-series-of-price-anomaly-detection-13586cd5ff46
isf = isolationForest$new()
isf$fit(dplyr::select(df,PM))
plot(density(isf$scores$anomaly_score),ylab = "Density",main="Anomaly Score")
df$scores = isf$scores
#assume
cutoff = quantile(isf$scores$anomaly_score,c(0.975))
plot.default(df$datetime, df$PM,col=ifelse(df$scores$anomaly_score>cutoff,"red","black"),ylab = "PM2.5 (ug/m^3)")
isfMV = isolationForest$new()
isfMV$fit(dplyr::select(df,PM,Wind,Temp))
plot(density(isfMV$scores$anomaly_score),ylab = "Density",main="Anomaly Score")
df$MVscores = isf$scores
#assume
cutoff = quantile(isf$scores$anomaly_score,c(0.975))
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df$scores$anomaly_score>cutoff,"red","black"))
plot(density(df$PM),main="PM Value Distribution")
df2$if_anomaly = ifelse(df$scores$anomaly_score>cutoff,1,0)
df
df2
View(df2)
df2$prob_anomaly = ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,1,0)
plot(density(df$PM),main="PM Value Distribution")
#also from https://towardsdatascience.com/time-series-of-price-anomaly-detection-13586cd5ff46
df_fit <- fitdist(data=df$PM[df$PM!=0], distr = "gamma", method = "mle")
df_fit$estimate
plot(density(df$PM),main="PM Value Density")
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
df$probs = pgamma(df$PM,shape=df_fit$estimate[1],rate=df_fit$estimate[2])
plot(df$datetime, df$PM,col=ifelse(df$probs<.025 | df$probs>0.975,"red","black"),ylab = "PM2.5 (ug/m^3)")
plot(density(df$Temp),main="Temp Value Density")
plot(density(df$Wind),main="Wind Value Density")
mvNorms = mvn(modelName="Ellipsoidal",data=dplyr::select(df,PM,Temp,Wind))
mvMeans = mvNorms$parameters$mean
mvVars = as.matrix(as.data.frame(mvNorms$parameters$variance$sigma),nrow=3,ncol=3)
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
mvQuant = quantile(mvNormProbs,c(0.025,0.975))
plot3d(dplyr::select(df,PM,Temp,Wind), col=ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,"red","black"))
df2$prob_anomaly = ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,1,0)
mvMeans = mvNorms$parameters$mean
mvVars = as.matrix(as.data.frame(mvNorms$parameters$variance$sigma),nrow=3,ncol=3)
mvNormProbs = dmvnorm(as.matrix(dplyr::select(df,PM,Temp,Wind)),mvMeans,mvVars)
mvQuant = quantile(mvNormProbs,c(0.025,0.975))
plot3d(dplyr::select(df,PM,Temp,Wind), col=ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,"red","black"))
df2$prob_anomaly = ifelse(mvNormProbs<mvQuant[1] | mvQuant[2]>0.975,1,0)
View(df2)
preds=data.frame(upper=numeric(),lower=numeric())
i=0
for (i in c(3:dim(df_train)[1]-1)){
modelExp1 <- holt(df_train$PM[c(1:i)], h=1, level=99,initial="optimal",beta=NULL, gamma=NULL)
predictions = predict(modelExp1, df_train$PM[i+1], prediction.interval = TRUE)
preds=rbind(preds,c(predictions$upper,predictions$lower))
}
# http://r-statistics.co/Time-Series-Forecasting-With-R.html
#do a moving average
df_movingAvg <- ma(df$PM, order=24) # 1 day moving average
plot(df_movingAvg, col="red",ylab ="Average PM in 24hr window",xlab = "index") # plot
df_movingAvg <- ma(df$PM, order=dim(df)[1]-10) # 12 month moving average
plot(df_movingAvg, col="red",ylab ="Average PM in the entire data set window",xlab = "index")
mean(df$PM)
# 1 day moving average with exponential smoothing
modelExp1 <- holt(df$PM, h=1, level=95,initial="optimal",beta=NULL, gamma=NULL)
plot(modelExp1,ylab ="Average PM in the window",xlab = "index")
preds=data.frame(upper=numeric(),lower=numeric())
i=0
for (i in c(3:dim(df)[1]-1)){
modelExp1 <- holt(df$PM[c(1:i)], h=1, level=99,initial="optimal",beta=NULL, gamma=NULL)
predictions = predict(modelExp1, df$PM[i+1], prediction.interval = TRUE)
preds=rbind(preds,c(predictions$upper,predictions$lower))
}
colnames(preds) = c("upper","lower")
df2$ts1_anomaly = ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,1,0)
df2$ts1_anomaly[c(1,2)]=c(0,0)
df2$ts1_anomaly=0
df2$ts1_anomaly[c(3:dim(df2$ts1_anomaly)[1])]=c(0,0) = ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,1,0)
df2
df2$ts1_anomaly[c(3:dim(df2$ts1_anomaly)[1])]
c(3:dim(df2$ts1_anomaly)[1])
dim(df2$ts1_anomaly)[1]
df2
dim(df2)[1]
df2$ts1_anomaly[c(3:dim(df2)[1])]=c(0,0) = ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,1,0)
df2$ts1_anomaly[c(3:dim(df2)[1])]
df2$ts1_anomaly[c(3:dim(df2)[1])]= ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,1,0)
df2
df2$ts1_anomaly=0
df2$ts1_anomaly[c(3:dim(df2)[1])] = ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,1,0)
plot(df$PM[c(0:500)],col=ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,"red","black"),pch=16,ylab ="PM2.5 (ug/m^3)",xlab = "index")
lines(preds$lower[c(0:500)],col="blue")
lines(preds$upper[c(0:500)],col="blue")
plot(df$PM,col=ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,"red","black"),pch=16,ylab="PM2.5 (ug/m^3)")
df2$ts1_anomaly=0
df2$ts1_anomaly[c(3:dim(df2)[1])] = ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,1,0)
plot(df$PM[c(0:500)],col=ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,"red","black"),ylab ="PM2.5 (ug/m^3)",xlab = "index")
lines(preds$lower[c(0:500)],col="blue")
lines(preds$upper[c(0:500)],col="blue")
plot(df$PM,col=ifelse(df$PM[c(3:dim(df))]<preds$lower | df$PM[c(3:dim(df))]>preds$upper,"red","black"),ylab="PM2.5 (ug/m^3)")
# https://towardsdatascience.com/tidy-anomaly-detection-using-r-82a0c776d523
df_tb = as.tibble(df)
decomposed_df = time_decompose(df_tb,target=PM, method = "stl", frequency = "auto", trend = "auto")
anomalized_df = anomalize(decomposed_df,target=remainder, method = "gesd", alpha = 0.05, max_anoms = 0.3)
plot_anomaly_decomposition(anomalized_df)
recomposed_df = time_recompose(anomalized_df)
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
recomposed_df
recomposed_df$anomaly
as.integer(recomposed_df$anomaly)
recomposed_df$anomaly=="yes"
recomposed_df$anomaly=="Yes"
df$ts_2=recomposed_df$anomaly=="Yes"
df2$ts_2=recomposed_df$anomaly=="Yes"
df2
df2$ts_2=as.integer(recomposed_df$anomaly=="Yes")
df2
View(df2)
df2$ts_2_anomaly=as.integer(recomposed_df$anomaly=="Yes")
df2_tot_anomaly = df2$km_anomaly+df2$svm_anomaly+df2$if_anomaly+df2$prob_anomaly+df2$ts1_anomaly+df2$ts_2_anomaly
df2$tot_anomaly = df2$km_anomaly+df2$svm_anomaly+df2$if_anomaly+df2$prob_anomaly+df2$ts1_anomaly+df2$ts_2_anomaly
df2$tot_anomaly
table(df2$tot_anomaly)
df2[df2$tot_anomaly>=3]
df2[df2$tot_anomaly>=3,]
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"red","black"),pch=ifelse(df2$tot_anomaly>=3,8,16))
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"red","black"),size=ifelse(df2$tot_anomaly>=3,20,10))
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"green","black"))
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"red","black"))
plot(dplyr::select(df,PM), col=ifelse(df2$tot_anomaly>=3,"red","black"))
plot(df$datetime,df$PM, col=ifelse(df2$tot_anomaly>=3,"red","black"))
plot(df$datetime,df$PM, col=ifelse(df2$tot_anomaly>=3,"red","black"),ylab="PM2.5 (ug/m^3)")
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"red","black"))
df2$tot_anomaly = df2$km_anomaly+df2$svm_anomaly+df2$if_anomaly+df2$prob_anomaly+df2$ts1_anomaly+df2$ts_2_anomaly
df2$tot_anomaly = df2$km_anomaly+df2$svm_anomaly+df2$if_anomaly+df2$prob_anomaly+df2$ts1_anomaly+df2$ts_2_anomaly
table(df2$tot_anomaly)
plot.default()
df2$tot_anomaly = df2$km_anomaly+df2$svm_anomaly+df2$if_anomaly+df2$prob_anomaly+df2$ts1_anomaly+df2$ts_2_anomaly
table(df2$tot_anomaly)
df2[df2$tot_anomaly>=3,]
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"red","black"))
plot(df$datetime,df$PM, col=ifelse(df2$tot_anomaly>=3,"red","black"),ylab="PM2.5 (ug/m^3)")
recomposed_df = time_recompose(anomalized_df)
df2$ts_2_anomaly=as.integer(recomposed_df$anomaly=="Yes")
plot_anomalies(recomposed_df,time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
df2$tot_anomaly = df2$km_anomaly+df2$svm_anomaly+df2$if_anomaly+df2$prob_anomaly+df2$ts1_anomaly+df2$ts_2_anomaly
table(df2$tot_anomaly)
df2[df2$tot_anomaly>=3,]
plot3d(dplyr::select(df,PM,Wind,Temp), col=ifelse(df2$tot_anomaly>=3,"red","black"))
plot(df$datetime,df$PM, col=ifelse(df2$tot_anomaly>=3,"red","black"),ylab="PM2.5 (ug/m^3)")
x
plot(density(df$PM),main="PM Value Density")
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
density(df$PM)
help(curve)
head(df2[df2$tot_anomaly>=3,])
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
plot(density(df$PM),main="PM Value Density")
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
plot(density(df$PM),main="PM Value Density")
curve(dgamma(density(df$PM), shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
curve(dgamma(df$PM, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
plot(density(df$PM),main="PM Value Density")
curve(dgamma(x, shape=df_fit$estimate[1], rate=df_fit$estimate[2]), 0,100, add=TRUE,
col="gray")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(rgl)
library(knitr)
rgl::setupKnitr()
knit_hooks$set(webgl = hook_webgl)
#This code will not run correctly unless you download the hourly data files from https://www.epa.gov/outdoor-air-quality-data for PM2.5, Temp, and Wind speed in 2019, unzip them, move the files into the local directory, and rename the files pm_data, temp_data, and wind_data respectively. This is all unnecessary as we have already performed this step and provide the epa.csv file with the final data.
#creating our csv
#load in pm
df = read.csv("./pm_data.csv")
install.packages('NHPoisson')
library(tidyverse)
library(tidyverse)
library(igraph)      # note the conflicts: simplify(), groups()
library(NHPoisson)
pm <- read.csv(file="C://Users//Jim//GITPROJECTS//Smart_Cville_Capstone_2020//code//pmData.csv", header=TRUE, sep=",")
# get just pm25 for dev box 3
pm_dev3_pm25 <- select(filter(pm, dev_id == 3),c(pm25))
pm_dev3_pm25
pm <- read.csv(file="C://Users//Jim//GITPROJECTS//Smart_Cville_Capstone_2020//code//pmData.csv", header=TRUE, sep=",")
pm <- read.csv(file="..//code//pmData.csv", header=TRUE, sep=",")
# get just pm25 for dev box 3
pm_dev3_pm25 <- select(filter(pm, dev_id == 3),c(pm25))
pm_dev3_pm25
pm <- read.csv(file="..//code//pmData.csv", header=TRUE, sep=",")
# get just pm25 for dev box 3
pm_dev3_pm25 <- select(filter(pm, dev_id == 3),c(pm25))
pm_dev3_pm25
plot(pm_dev3_pm25)
hist(pm_dev3_pm25)
setwd "C:\Users\Jime\Documents\gitFolders\Smart_Cville_Capstone_2020\data\tl_2019_51540_faces"
setwd("C:\Users\Jime\Documents\gitFolders\Smart_Cville_Capstone_2020\data\tl_2019_51540_faces")
setwd("C:\\Users\\Jime\\Documents\\gitFolders\\Smart_Cville_Capstone_2020\\data\\tl_2019_51540_faces")
a = extractCoords(shp)
install.packages(c("automap", "raster", "rgdal", "spatstat"))
library(automap)
library(kriging)
library(rgdal)
library(spatstat)
library(maptools)
library(raster)
library(sp)
df <- read.csv("pre_means.csv")
df2 <- df
data = df[,4:6]
shape <- readOGR("tl_2019_51540_faces.shp")
shp = shape@polygons
shp = SpatialPolygons(shp, proj4string=CRS("+proj=utm +ellps=WGS84 +datum=WGS84"))
extractCoords <- function(sp.df)
{
results <- list()
for(i in 1:length(sp.df@polygons[[1]]@Polygons))
{
for(j in 1:1014)
{
results[[j]] <- sp.df@polygons[[j]]@Polygons[[1]]@coords
}
}
results <- Reduce(rbind, results)
results
}
a = extractCoords(shp)
coordinates(df) <- ~ longitude + latitude
spdf = SpatialPointsDataFrame(df@coords, data)
spdf@data
proj4string(spdf) <- proj4string(r)
sp_pts <- SpatialPoints(a)
kriging_pm25 = autoKrige(spdf$pm25~1, spdf, sp_pts, model = "Exp")
plot(kriging_pm25)
kriging_pm10 = autoKrige(spdf$pm10~1, spdf, sp_pts, model = "Exp")
plot(kriging_pm10)
kriging_pm25
plot(kriging_pm25)
plot(kriging_pm25, sp.layout = list(pts = list("sp.points", spdf)))
kriging_pm25$krige_output
plot(kriging_pm25$krige_output, sp.layout = list(pts = list("sp.points", spdf)))
kriging_pm25$var_model
plot(kriging_pm25$krige_output)
plot(kriging_pm10$krige_output)
plot(kriging_pm25$krige_output)
plot(kriging_pm25)
df_pre <- read.csv("pre_means.csv")
df_post <- read.csv("post_means.csv")
data_pre = df_pre[,4:6]
data_post = df_post[,4:6]
shape <- readOGR("tl_2019_51540_faces.shp")
shp = shape@polygons
shp = SpatialPolygons(shp, proj4string=CRS("+proj=utm +ellps=WGS84 +datum=WGS84"))
extractCoords <- function(sp.df)
{
results <- list()
for(i in 1:length(sp.df@polygons[[1]]@Polygons))
{
for(j in 1:1014)
{
results[[j]] <- sp.df@polygons[[j]]@Polygons[[1]]@coords
}
}
results <- Reduce(rbind, results)
results
}
a = extractCoords(shp)
coordinates(df_pre) <- ~ longitude + latitude
coordinates(df_post) <- ~ longitude + latitude
spdf_pre = SpatialPointsDataFrame(df_pre@coords, data)
spdf_post = SpatialPointsDataFrame(df_post@coords, data)
spdf_pre@data
spdf_post@data
proj4string(spdf_pre) <- proj4string(r)
sp_pts <- SpatialPoints(a)
kriging_pm25 = autoKrige(spdf_pre$pm25~1, spdf_pre, sp_pts, model = "Exp")
plot(kriging_pm25)
kriging_co2_pre = autoKrige(spdf_pre$co2~1, spdf_pre, sp_pts, model = "Exp")
plot(kriging_co2_pre)
kriging_co2_post = autoKrige(spdf_post$co2~1, spdf_post, sp_pts, model = "Exp")
plot(kriging_co2_post)
kriging_co2_pre = autoKrige(spdf_pre$co2~1, spdf_pre, sp_pts, model = "Exp")
plot(kriging_co2_pre)
